# ğŸ§  Paper Critic Agent (Hybrid LLM)

A lightweight, agentic RAG project that critiques research papers using multiple LLM backends. **No more manual PDF downloads** â€” just paste an arXiv URL and get instant, detailed academic critiques. Choose between local models (Hugging Face), free APIs (Groq), or Ollama. No LangChain, no LlamaIndex â€” just pure Python.

## ğŸš€ Key Features

- ğŸ“ **Direct arXiv Integration** - Paste URLs, get instant analysis
- ğŸ¤– **Hybrid LLM Backends** - Auto-selects best available option
- ğŸ§¾ **Smart PDF Processing** - Extracts abstracts, methods, and metadata
- ğŸ” **Contextual Paper Retrieval** - Finds related work using paper titles
- ğŸ§  **Academic-Grade Critiques** - Structured, detailed methodology reviews
- ğŸ” **Self-Reflection Loop** - Revises and improves initial critiques
- ğŸ”„ **Intelligent Fallbacks** - Never fails due to single backend issues
- ğŸ§¹ **Auto-Cleanup** - Manages temporary files automatically

## ğŸ¯ LLM Backend Options

### Option 1: Groq API (Recommended - Fast & Free)
1. **Get free API key**: Sign up at [console.groq.com](https://console.groq.com)
2. **Set environment variable**:
```bash
export GROQ_API_KEY="your_api_key_here"
```

### Option 2: Hugging Face Transformers (Local & Private)
1. **Install transformers**:
```bash
pip install transformers torch
```

### Option 3: Ollama (Original)
1. **Install Ollama**: https://ollama.com/download
2. **Pull a model**:
```bash
ollama pull llama3
```

## ğŸ“¦ Quick Start

### 1. Clone and Setup
```bash
git clone https://github.com/josephbak/paper-critic-agent.git
cd paper-critic-agent

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Choose Your LLM Backend
Pick one of the options above (Groq recommended for beginners)

### 3. Run the Agent
```bash
# With arXiv URL (recommended)
python main.py "https://arxiv.org/abs/2301.07041"

# With arXiv ID
python main.py "2301.07041"

# With local PDF
python main.py "path/to/paper.pdf"

# Default (uses GPT-4 paper as example)
python main.py
```

The script will automatically detect which backend is available and use the best option.

## ğŸ”§ Advanced Usage

You can also specify the backend manually:

```python
# Force specific backend
critique = run_llm(prompt, backend="groq")
critique = run_llm(prompt, backend="huggingface") 
critique = run_llm(prompt, backend="ollama")

# Use different models
critique = run_llm(prompt, backend="groq", model="mixtral-8x7b-32768")
critique = run_llm(prompt, backend="huggingface", model="microsoft/DialoGPT-large")
```

### Recommended Models by Backend:
- **Groq**: `llama3-8b-8192` (default), `mixtral-8x7b-32768`, `gemma-7b-it`
- **Hugging Face**: `microsoft/DialoGPT-medium` (default), `microsoft/DialoGPT-large`
- **Ollama**: `llama3` (default), `mistral`, `codellama`

## ğŸ“‹ Example Output

```
ğŸ” Detected arXiv ID: 1706.03762
ğŸ“‹ Fetching paper metadata...
ğŸ“„ Title: Attention Is All You Need

ğŸ¤– Step 1: Extracting abstract & method...
ğŸ“ Abstract length: 2259 chars
ğŸ“ Method length: 2143 chars

ğŸ¤– Step 2: Retrieving similar papers from arXiv...
ğŸ“š Retrieved related papers.

ğŸ¤– Step 3: Critiquing the paper...
ğŸ“ Critique Result:

**Methodology Clarity**: The methodology is partially clear...
**Experimental Flaws**: There are some concerns regarding experimental design...
**Methodology Soundness**: While the Transformer architecture is interesting...

ğŸ” Reflecting and revising critique...
[Detailed structured critique with specific recommendations]
```

## ğŸ”’ Privacy & Security

- **Local Models (HF/Ollama)**: 100% private, nothing leaves your machine
- **Groq API**: Data sent to Groq servers (check their privacy policy)
- **arXiv Downloads**: Temporary files auto-deleted after processing
- **Git Safety**: Models stored outside project, won't be committed

## ğŸ›  Troubleshooting

**"Groq API Error"**: Check your API key in `.env` file
**"Ollama not found"**: Make sure Ollama service is running (`ollama serve`)
**"No method section found"**: Try papers with clear "Method" or "Methodology" sections
**"NumPy compatibility"**: Run `pip install "numpy<2"` if you see warnings

## âœ… Next Steps

- [ ] ğŸ”Œ Add XML parsing of arXiv output to extract metadata
- [ ] ğŸ“Š Use vector DB (like FAISS) for deeper similarity comparison
- [ ] ğŸ–¼ Add Streamlit UI to upload a paper and display critique
- [ ] ğŸ§  Let LLM generate search query from abstract
- [ ] ğŸ“‘ Save critique as a formatted PDF or Markdown report
